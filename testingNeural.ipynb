{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class SquaredErrorLoss(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return 0.5*np.sum(\n",
    "            (predicted - actual) ** 2\n",
    "        )\n",
    "    \n",
    "    def delta(self, z_prime, predicted, actual):\n",
    "        return (\n",
    "            np.multiply((predicted - actual), z_prime.T)\n",
    "        )\n",
    "\n",
    "class crossEntropy(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return np.sum(\n",
    "            np.nan_to_num(-actual*np.log(predicted)-(1-actual)*np.log(1-predicted))\n",
    "        )\n",
    "    \n",
    "    def delta(self, z_prime, predicted, actual):\n",
    "        return (\n",
    "            (predicted-actual)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return np.ones(h.shape)\n",
    "    \n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return np.where(a > 0, a, 0)\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return np.where(a > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return 1/(1 + np.exp(-a))\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        e = self.__call__(a)\n",
    "        return  np.multiply(e, (1 - e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "    \n",
    "    def get_layer_derivatives(self, values_pre_activation):\n",
    "        return self.activation_function.derivative(\n",
    "            values_pre_activation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.updatedWeights = []\n",
    "        self.biases = []\n",
    "        self.updatedBiases = []\n",
    "        mu, sigma = 0, 1\n",
    "        for i in range(1, len(self.layers)):\n",
    "            w = np.matrix(np.random.normal(mu, sigma, (self.layers[i - 1].num_nodes, self.layers[i].num_nodes)))\n",
    "            self.weights.append(w)\n",
    "            self.updatedWeights.append(w)\n",
    "            self.biases.append(np.zeros(self.layers[i].num_nodes))\n",
    "            self.updatedBiases.append(np.zeros(self.layers[i].num_nodes))\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        self.storedValuesZ = [inputs]\n",
    "        self.storedValuesA = [inputs]\n",
    "        a = inputs\n",
    "        \n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                print('a',a)\n",
    "                z = np.matrix(np.add(a * self.weights[i], np.matrix(self.biases[i])))\n",
    "                self.storedValuesZ.append(z)\n",
    "                a = np.matrix(self.layers[i + 1].get_layer_values(z))\n",
    "                self.storedValuesA.append(a)\n",
    "        return a\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        h = inputs\n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                a = np.matrix(np.add(h * self.updatedWeights[i], np.matrix(self.updatedBiases[i])))\n",
    "                h = self.layers[i+1].get_layer_values(a)\n",
    "        return h\n",
    "    \n",
    "    def backProp(self, predicted, actual):\n",
    "        # Update First weights\n",
    "        z_prime = 0\n",
    "        if (type(self.loss) == type(SquaredErrorLoss())):\n",
    "            z_prime = self.layers[-1].get_layer_derivatives(self.storedValuesZ[-1]).T\n",
    "        delta = self.loss.delta(z_prime, predicted, actual).T\n",
    "        dLdw = np.multiply(delta,self.storedValuesA[-2]).T\n",
    "        \n",
    "        self.updatedWeights[-1]= self.weights[-1] - self.learning_rate * dLdw\n",
    "        self.updatedBiases[-1] = self.biases[-1] - np.multiply(self.learning_rate, delta).T\n",
    "        \n",
    "        # Update rest of the weights\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = self.storedValuesZ[-l]\n",
    "            dadz = self.layers[-l].get_layer_derivatives(z)\n",
    "            delta = np.multiply(self.weights[-l + 1] * delta, dadz.T)\n",
    "            self.updatedBiases[-l] = self.biases[-l] - np.multiply(self.learning_rate, delta).T\n",
    "            self.updatedWeights[-l] = self.weights[-l] - np.multiply(self.learning_rate, np.dot(delta, self.storedValuesA[-l - 1])).T\n",
    "        self.weights = self.updatedWeights\n",
    "        self.biases = self.updatedBiases\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "        self.backProp(predicted, labels)\n",
    "        return predicted\n",
    "    \n",
    "    def train_epochs(self, inputs, labels, epochs = 50):\n",
    "        randomIndices = np.random.choice([i for i in range(len(inputs))], size=len(inputs), replace=False)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for index in randomIndices:\n",
    "                self.train(np.matrix([inputs[index]]),np.matrix([labels[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(1), Layer(1),Layer(3)],\n",
    "    loss = crossEntropy(),\n",
    "    learning_rate=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[1]]\n",
      "a [[ 0.47245588]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.00115113,  1.99889903,  2.99682001]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.feedforward(np.matrix([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[1]]\n",
      "a [[ 0.41008928]]\n",
      "a [[1]]\n",
      "a [[ 0.41686078]]\n",
      "a [[1]]\n",
      "a [[ 0.4229586]]\n",
      "a [[1]]\n",
      "a [[ 0.42843672]]\n",
      "a [[1]]\n",
      "a [[ 0.43334762]]\n",
      "a [[1]]\n",
      "a [[ 0.43774167]]\n",
      "a [[1]]\n",
      "a [[ 0.44166657]]\n",
      "a [[1]]\n",
      "a [[ 0.44516712]]\n",
      "a [[1]]\n",
      "a [[ 0.44828497]]\n",
      "a [[1]]\n",
      "a [[ 0.45105864]]\n",
      "a [[1]]\n",
      "a [[ 0.4535235]]\n",
      "a [[1]]\n",
      "a [[ 0.45571184]]\n",
      "a [[1]]\n",
      "a [[ 0.45765308]]\n",
      "a [[1]]\n",
      "a [[ 0.45937381]]\n",
      "a [[1]]\n",
      "a [[ 0.4608981]]\n",
      "a [[1]]\n",
      "a [[ 0.46224758]]\n",
      "a [[1]]\n",
      "a [[ 0.46344167]]\n",
      "a [[1]]\n",
      "a [[ 0.4644978]]\n",
      "a [[1]]\n",
      "a [[ 0.46543152]]\n",
      "a [[1]]\n",
      "a [[ 0.46625672]]\n",
      "a [[1]]\n",
      "a [[ 0.46698579]]\n",
      "a [[1]]\n",
      "a [[ 0.46762975]]\n",
      "a [[1]]\n",
      "a [[ 0.4681984]]\n",
      "a [[1]]\n",
      "a [[ 0.46870043]]\n",
      "a [[1]]\n",
      "a [[ 0.46914356]]\n",
      "a [[1]]\n",
      "a [[ 0.46953463]]\n",
      "a [[1]]\n",
      "a [[ 0.46987972]]\n",
      "a [[1]]\n",
      "a [[ 0.47018419]]\n",
      "a [[1]]\n",
      "a [[ 0.47045278]]\n",
      "a [[1]]\n",
      "a [[ 0.4706897]]\n",
      "a [[1]]\n",
      "a [[ 0.47089867]]\n",
      "a [[1]]\n",
      "a [[ 0.47108297]]\n",
      "a [[1]]\n",
      "a [[ 0.4712455]]\n",
      "a [[1]]\n",
      "a [[ 0.47138883]]\n",
      "a [[1]]\n",
      "a [[ 0.47151521]]\n",
      "a [[1]]\n",
      "a [[ 0.47162664]]\n",
      "a [[1]]\n",
      "a [[ 0.47172489]]\n",
      "a [[1]]\n",
      "a [[ 0.47181152]]\n",
      "a [[1]]\n",
      "a [[ 0.47188789]]\n",
      "a [[1]]\n",
      "a [[ 0.47195522]]\n",
      "a [[1]]\n",
      "a [[ 0.47201457]]\n",
      "a [[1]]\n",
      "a [[ 0.4720669]]\n",
      "a [[1]]\n",
      "a [[ 0.47211303]]\n",
      "a [[1]]\n",
      "a [[ 0.47215369]]\n",
      "a [[1]]\n",
      "a [[ 0.47218954]]\n",
      "a [[1]]\n",
      "a [[ 0.47222114]]\n",
      "a [[1]]\n",
      "a [[ 0.47224899]]\n",
      "a [[1]]\n",
      "a [[ 0.47227354]]\n",
      "a [[1]]\n",
      "a [[ 0.47229519]]\n",
      "a [[1]]\n",
      "a [[ 0.47231426]]\n",
      "a [[1]]\n",
      "a [[ 0.47233108]]\n",
      "a [[1]]\n",
      "a [[ 0.4723459]]\n",
      "a [[1]]\n",
      "a [[ 0.47235897]]\n",
      "a [[1]]\n",
      "a [[ 0.47237049]]\n",
      "a [[1]]\n",
      "a [[ 0.47238064]]\n",
      "a [[1]]\n",
      "a [[ 0.47238959]]\n",
      "a [[1]]\n",
      "a [[ 0.47239747]]\n",
      "a [[1]]\n",
      "a [[ 0.47240443]]\n",
      "a [[1]]\n",
      "a [[ 0.47241055]]\n",
      "a [[1]]\n",
      "a [[ 0.47241595]]\n",
      "a [[1]]\n",
      "a [[ 0.47242072]]\n",
      "a [[1]]\n",
      "a [[ 0.47242491]]\n",
      "a [[1]]\n",
      "a [[ 0.47242861]]\n",
      "a [[1]]\n",
      "a [[ 0.47243187]]\n",
      "a [[1]]\n",
      "a [[ 0.47243474]]\n",
      "a [[1]]\n",
      "a [[ 0.47243728]]\n",
      "a [[1]]\n",
      "a [[ 0.47243951]]\n",
      "a [[1]]\n",
      "a [[ 0.47244148]]\n",
      "a [[1]]\n",
      "a [[ 0.47244321]]\n",
      "a [[1]]\n",
      "a [[ 0.47244474]]\n",
      "a [[1]]\n",
      "a [[ 0.47244609]]\n",
      "a [[1]]\n",
      "a [[ 0.47244728]]\n",
      "a [[1]]\n",
      "a [[ 0.47244832]]\n",
      "a [[1]]\n",
      "a [[ 0.47244925]]\n",
      "a [[1]]\n",
      "a [[ 0.47245006]]\n",
      "a [[1]]\n",
      "a [[ 0.47245078]]\n",
      "a [[1]]\n",
      "a [[ 0.47245141]]\n",
      "a [[1]]\n",
      "a [[ 0.47245197]]\n",
      "a [[1]]\n",
      "a [[ 0.47245246]]\n",
      "a [[1]]\n",
      "a [[ 0.47245289]]\n",
      "a [[1]]\n",
      "a [[ 0.47245327]]\n",
      "a [[1]]\n",
      "a [[ 0.47245361]]\n",
      "a [[1]]\n",
      "a [[ 0.4724539]]\n",
      "a [[1]]\n",
      "a [[ 0.47245416]]\n",
      "a [[1]]\n",
      "a [[ 0.47245439]]\n",
      "a [[1]]\n",
      "a [[ 0.4724546]]\n",
      "a [[1]]\n",
      "a [[ 0.47245478]]\n",
      "a [[1]]\n",
      "a [[ 0.47245493]]\n",
      "a [[1]]\n",
      "a [[ 0.47245507]]\n",
      "a [[1]]\n",
      "a [[ 0.4724552]]\n",
      "a [[1]]\n",
      "a [[ 0.4724553]]\n",
      "a [[1]]\n",
      "a [[ 0.4724554]]\n",
      "a [[1]]\n",
      "a [[ 0.47245548]]\n",
      "a [[1]]\n",
      "a [[ 0.47245556]]\n",
      "a [[1]]\n",
      "a [[ 0.47245562]]\n",
      "a [[1]]\n",
      "a [[ 0.47245568]]\n",
      "a [[1]]\n",
      "a [[ 0.47245573]]\n",
      "a [[1]]\n",
      "a [[ 0.47245577]]\n",
      "a [[1]]\n",
      "a [[ 0.47245581]]\n",
      "a [[1]]\n",
      "a [[ 0.47245585]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "  network.train(np.matrix([[1]]), np.matrix([[1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABV</th>\n",
       "      <th>appearance</th>\n",
       "      <th>aroma</th>\n",
       "      <th>overall</th>\n",
       "      <th>palate</th>\n",
       "      <th>taste</th>\n",
       "      <th>userBias</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>...</th>\n",
       "      <th>Slovakia</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Sri Lanka</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>Switzerland</th>\n",
       "      <th>Taiwan, Province of China</th>\n",
       "      <th>Thailand</th>\n",
       "      <th>Togo</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>United States</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092014</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.135916</td>\n",
       "      <td>0.312620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.192708</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.766250</td>\n",
       "      <td>0.238418</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.270866</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>0.119221</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.213559</td>\n",
       "      <td>0.221747</td>\n",
       "      <td>0.344344</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175347</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.209332</td>\n",
       "      <td>0.288806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ABV  appearance  aroma  overall  palate  taste  userBias  word_count  \\\n",
       "0  0.092014         0.7  0.750      1.0   0.750  0.875  0.745000    0.135593   \n",
       "1  0.192708         0.8  1.000      1.0   1.000  1.000  0.766250    0.238418   \n",
       "2  0.111111         0.8  0.875      1.0   0.875  0.875  0.775000    0.114124   \n",
       "3  0.123264         0.8  0.875      1.0   0.750  0.875  0.711000    0.213559   \n",
       "4  0.175347         0.8  0.875      1.0   0.875  0.875  0.755556    0.211299   \n",
       "\n",
       "   char_count  avg_word_len      ...        Slovakia  Spain  Sri Lanka  \\\n",
       "0    0.135916      0.312620      ...             0.0    0.0        0.0   \n",
       "1    0.232877      0.270866      ...             NaN    NaN        NaN   \n",
       "2    0.119221      0.425918      ...             NaN    NaN        NaN   \n",
       "3    0.221747      0.344344      ...             NaN    NaN        NaN   \n",
       "4    0.209332      0.288806      ...             NaN    NaN        NaN   \n",
       "\n",
       "   Sweden  Switzerland  Taiwan, Province of China  Thailand  Togo  \\\n",
       "0     0.0          0.0                        0.0       0.0   0.0   \n",
       "1     NaN          NaN                        NaN       NaN   NaN   \n",
       "2     NaN          NaN                        NaN       NaN   NaN   \n",
       "3     NaN          NaN                        NaN       NaN   NaN   \n",
       "4     NaN          NaN                        NaN       NaN   NaN   \n",
       "\n",
       "   United Kingdom  United States  \n",
       "0             0.0            0.0  \n",
       "1             NaN            NaN  \n",
       "2             NaN            NaN  \n",
       "3             NaN            NaN  \n",
       "4             NaN            NaN  \n",
       "\n",
       "[5 rows x 185 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ABV', 'appearance', 'aroma', 'overall', 'palate', 'taste', 'userBias',\n",
       "       'word_count', 'char_count', 'avg_word_len',\n",
       "       ...\n",
       "       'Slovakia', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland',\n",
       "       'Taiwan, Province of China', 'Thailand', 'Togo', 'United Kingdom',\n",
       "       'United States'],\n",
       "      dtype='object', length=185)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXandY(data, dependentNames, independentName, intercept):\n",
    "    X = np.array(data[dependentNames])\n",
    "    if (intercept):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    y = np.array(data[independentName])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = makeXandY(test, ['ABV'], ['appearance'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempX = x[:100]\n",
    "tempY = y[:100]\n",
    "network.train_epochs(tempX, tempY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.87188217]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.feedforward(np.matrix(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.88438475]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.feedforward(np.matrix(x[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
