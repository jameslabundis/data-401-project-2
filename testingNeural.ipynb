{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class SquaredErrorLoss(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return 0.5*np.sum(\n",
    "            np.multiply((predicted - actual), (predicted - actual))\n",
    "        )\n",
    "    \n",
    "    def delta(self, z_prime, predicted, actual):\n",
    "        return (\n",
    "            np.multiply((predicted - actual), z_prime.T)\n",
    "        )\n",
    "\n",
    "class crossEntropy(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return np.sum(\n",
    "            np.nan_to_num(np.multiply(-actual,np.log(predicted))-np.multiply((1-actual),np.log(1-predicted)))\n",
    "        )\n",
    "    \n",
    "    def delta(self, z_prime, predicted, actual):\n",
    "        return (\n",
    "            (predicted-actual)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return np.ones(h.shape)\n",
    "    \n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return np.where(a > 0, a, 0)\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return np.where(a > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return 1/(1 + np.exp(-a))\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        e = self.__call__(a)\n",
    "        return  np.multiply(e, (1 - e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "    \n",
    "    def get_layer_derivatives(self, values_pre_activation):\n",
    "        return self.activation_function.derivative(\n",
    "            values_pre_activation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        mu, sigma = 0, 1\n",
    "        for i in range(1, len(self.layers)):\n",
    "            w = np.matrix(np.random.normal(mu, sigma, (self.layers[i - 1].num_nodes, self.layers[i].num_nodes))/np.sqrt(self.layers[i - 1].num_nodes))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(np.zeros(self.layers[i].num_nodes))\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        self.storedValuesZ = [inputs]\n",
    "        self.storedValuesA = [inputs]\n",
    "        a = inputs\n",
    "        \n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                z = np.matrix(np.add(a * self.weights[i], np.matrix(self.biases[i])))\n",
    "                self.storedValuesZ.append(z)\n",
    "                a = np.matrix(self.layers[i + 1].get_layer_values(z))\n",
    "                self.storedValuesA.append(a)\n",
    "        return a\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        h = inputs\n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                a = np.matrix(np.add(h * self.weights[i], np.matrix(self.biases[i])))\n",
    "                h = self.layers[i+1].get_layer_values(a)\n",
    "        return h\n",
    "    \n",
    "    def backProp(self, predicted, actual):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Update First weights\n",
    "        z_prime = 0\n",
    "        if (type(self.loss) == type(SquaredErrorLoss())):\n",
    "            z_prime = self.layers[-1].get_layer_derivatives(self.storedValuesZ[-1]).T\n",
    "        delta = self.loss.delta(z_prime, predicted, actual).T\n",
    "        dLdw = np.multiply(delta,self.storedValuesA[-2]).T\n",
    "        \n",
    "        gradient_b[-1] = delta.T\n",
    "        gradient_w[-1] = dLdw\n",
    "        #self.updatedWeights[-1]= self.weights[-1] - self.learning_rate * dLdw\n",
    "        #self.updatedBiases[-1] = self.biases[-1] - np.multiply(self.learning_rate, delta).T\n",
    "        \n",
    "        # Update rest of the weights\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = self.storedValuesZ[-l]\n",
    "            dadz = self.layers[-l].get_layer_derivatives(z)\n",
    "            delta = np.multiply(self.weights[-l + 1] * delta, dadz.T)\n",
    "            dLdw = np.multiply(delta, self.storedValuesA[-l - 1]).T\n",
    "            gradient_b[-l] = delta.T\n",
    "            gradient_w[-l] = dLdw\n",
    "            #self.updatedBiases[-l] = self.biases[-l] - np.multiply(self.learning_rate, delta).T\n",
    "            #self.updatedWeights[-l] = self.weights[-l] - np.multiply(self.learning_rate, np.dot(delta, self.storedValuesA[-l - 1])).T\n",
    "        return (gradient_b, gradient_w)\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "        gradient_b, gradient_w = self.backProp(predicted, labels)\n",
    "        return (gradient_b, gradient_w, self.loss(predicted,labels))\n",
    "    \n",
    "    def train_epochs_minibatch(self, inputs, labels, epochs = 10, mini_batch=1):\n",
    "        '''\n",
    "        Args:\n",
    "          inputs (np.ndarray): A x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the inputs size.\n",
    "          epochs (int): Number of times the data is iterated through\n",
    "          mini_batch (int): Number of observations to train at a time\n",
    "        '''\n",
    "        if (inputs.shape[0] < mini_batch):\n",
    "            mini_batch = inputs.shape[0]\n",
    "        meanLossEpochs = []\n",
    "        for i in range(epochs):\n",
    "            print('Epoch#:',i)\n",
    "            sum_gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            sum_gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            sumLoss = 0\n",
    "            randomIndices = np.random.choice([i for i in range(len(inputs))], size=inputs.shape[0], replace=False)\n",
    "            for index, row in enumerate(randomIndices):\n",
    "                gradient_b, gradient_w, loss = self.train(np.matrix([inputs[row]]),np.matrix([labels[row]]))\n",
    "                sumLoss += loss\n",
    "                sum_gradient_b = [sb+gb for sb, gb in zip(sum_gradient_b,gradient_b)]\n",
    "                sum_gradient_w = [sw+gw for sw, gw in zip(sum_gradient_w, gradient_w)]\n",
    "                if ((index+1)%mini_batch == 0 or (index+1) == inputs.shape[0]):\n",
    "                    self.biases = [b-(self.learning_rate/mini_batch)*sb\n",
    "                                      for b, sb in zip(self.biases, sum_gradient_b)]\n",
    "                    self.weights = [w-(self.learning_rate/mini_batch)*sw\n",
    "                                     for w, sw in zip(self.weights, sum_gradient_w)]\n",
    "                    sum_gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "                    sum_gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            avg_loss = sumLoss/inputs.shape[0]\n",
    "            meanLossEpochs.append(avg_loss)\n",
    "        return (meanLossEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid must be applied to the last layer inorder for it to work\n",
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(1), Layer(2, ReLU()), Layer(5, Sigmoid())],\n",
    "    loss = crossEntropy(),\n",
    "    learning_rate=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABV</th>\n",
       "      <th>appearance</th>\n",
       "      <th>aroma</th>\n",
       "      <th>overall</th>\n",
       "      <th>palate</th>\n",
       "      <th>taste</th>\n",
       "      <th>userBias</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>...</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Sri Lanka</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>Switzerland</th>\n",
       "      <th>Taiwan, Province of China</th>\n",
       "      <th>Thailand</th>\n",
       "      <th>Togo</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>United States</th>\n",
       "      <th>Location Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092014</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.135916</td>\n",
       "      <td>0.312620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.192708</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.766250</td>\n",
       "      <td>0.238418</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.270866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>0.119221</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.213559</td>\n",
       "      <td>0.221747</td>\n",
       "      <td>0.344344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175347</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.209332</td>\n",
       "      <td>0.288806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ABV  appearance  aroma  overall  palate  taste  userBias  word_count  \\\n",
       "0  0.092014         0.7  0.750      1.0   0.750  0.875  0.745000    0.135593   \n",
       "1  0.192708         0.8  1.000      1.0   1.000  1.000  0.766250    0.238418   \n",
       "2  0.111111         0.8  0.875      1.0   0.875  0.875  0.775000    0.114124   \n",
       "3  0.123264         0.8  0.875      1.0   0.750  0.875  0.711000    0.213559   \n",
       "4  0.175347         0.8  0.875      1.0   0.875  0.875  0.755556    0.211299   \n",
       "\n",
       "   char_count  avg_word_len        ...         Spain  Sri Lanka  Sweden  \\\n",
       "0    0.135916      0.312620        ...           0.0        0.0     0.0   \n",
       "1    0.232877      0.270866        ...           0.0        0.0     0.0   \n",
       "2    0.119221      0.425918        ...           0.0        0.0     0.0   \n",
       "3    0.221747      0.344344        ...           0.0        0.0     0.0   \n",
       "4    0.209332      0.288806        ...           0.0        0.0     0.0   \n",
       "\n",
       "   Switzerland  Taiwan, Province of China  Thailand  Togo  United Kingdom  \\\n",
       "0          0.0                        0.0       0.0   0.0             0.0   \n",
       "1          0.0                        0.0       0.0   0.0             0.0   \n",
       "2          0.0                        0.0       0.0   0.0             0.0   \n",
       "3          0.0                        0.0       0.0   0.0             0.0   \n",
       "4          0.0                        0.0       0.0   0.0             0.0   \n",
       "\n",
       "   United States  Location Unknown  \n",
       "0            0.0               0.0  \n",
       "1            0.0               1.0  \n",
       "2            0.0               1.0  \n",
       "3            0.0               1.0  \n",
       "4            0.0               1.0  \n",
       "\n",
       "[5 rows x 184 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependents = ['appearance','aroma','overall','palate','taste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "independents = []\n",
    "for x in list(test.columns):\n",
    "    if (x not in dependents):\n",
    "        independents.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "independents = ['userBias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXandY(data, dependentNames, independentName, intercept):\n",
    "    X = np.array(data[dependentNames])\n",
    "    if (intercept):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    y = np.array(data[independentName])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = makeXandY(test, independents, dependents, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 1)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 5)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = x[:100]\n",
    "testY = y[:100]\n",
    "trainX = x[100:1000]\n",
    "trainY = y[100:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch#: 0\n",
      "Epoch#: 1\n",
      "Epoch#: 2\n",
      "Epoch#: 3\n",
      "Epoch#: 4\n",
      "Epoch#: 5\n",
      "Epoch#: 6\n",
      "Epoch#: 7\n",
      "Epoch#: 8\n",
      "Epoch#: 9\n"
     ]
    }
   ],
   "source": [
    "epochsMeanError = network.train_epochs_minibatch(trainX, trainY, epochs=10, mini_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7  ,  0.75 ,  1.   ,  0.75 ,  0.875])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.87259022,  0.87222825,  0.99930779,  0.88324609,  0.92021554],\n",
       "        [ 0.87500957,  0.87464701,  0.99938885,  0.88613176,  0.92322206],\n",
       "        [ 0.87599428,  0.87563151,  0.99941941,  0.8873018 ,  0.92442958],\n",
       "        [ 0.86863616,  0.86827545,  0.99915517,  0.8784971 ,  0.91518099],\n",
       "        [ 0.87379695,  0.87343466,  0.99934932,  0.88468735,  0.92172219],\n",
       "        [ 0.89432632,  0.89396461,  0.99979188,  0.90858118,  0.94517911],\n",
       "        [ 0.87831202,  0.87794889,  0.99948615,  0.89004542,  0.92723474],\n",
       "        [ 0.87649411,  0.87613125,  0.9994344 ,  0.8878947 ,  0.92503893],\n",
       "        [ 0.87976745,  0.87940416,  0.99952457,  0.89176074,  0.92896956],\n",
       "        [ 0.87027677,  0.86991549,  0.99922172,  0.88047241,  0.91728805],\n",
       "        [ 0.88934728,  0.88898435,  0.99972099,  0.90290018,  0.93987089],\n",
       "        [ 0.86897613,  0.8686153 ,  0.99916935,  0.87890699,  0.91561973],\n",
       "        [ 0.89674101,  0.89638015,  0.99982026,  0.91130814,  0.94766473],\n",
       "        [ 0.87302687,  0.87266477,  0.99932308,  0.88376804,  0.92076233],\n",
       "        [ 0.87273669,  0.87237467,  0.99931295,  0.88342123,  0.92039916],\n",
       "        [ 0.8669469 ,  0.86658685,  0.99908154,  0.87645613,  0.91298471],\n",
       "        [ 0.89674101,  0.89638015,  0.99982026,  0.91130814,  0.94766473],\n",
       "        [ 0.87316306,  0.87280093,  0.99932778,  0.88393073,  0.92093249],\n",
       "        [ 0.88095377,  0.88059038,  0.99955402,  0.89315453,  0.93036833],\n",
       "        [ 0.86843602,  0.86807539,  0.99914673,  0.87825567,  0.91492219],\n",
       "        [ 0.87599428,  0.87563151,  0.99941941,  0.8873018 ,  0.92442958],\n",
       "        [ 0.88110836,  0.88074497,  0.99955774,  0.89333587,  0.93054961],\n",
       "        [ 0.87954233,  0.87917906,  0.99951879,  0.89149581,  0.92870257],\n",
       "        [ 0.87316306,  0.87280093,  0.99932778,  0.88393073,  0.92093249],\n",
       "        [ 0.86669705,  0.8663371 ,  0.99907019,  0.87615365,  0.91265756],\n",
       "        [ 0.89674101,  0.89638015,  0.99982026,  0.91130814,  0.94766473],\n",
       "        [ 0.89186198,  0.89149957,  0.99975903,  0.90577904,  0.94258258],\n",
       "        [ 0.86433647,  0.86397757,  0.99895678,  0.87328827,  0.90953766],\n",
       "        [ 0.87316306,  0.87280093,  0.99932778,  0.88393073,  0.92093249],\n",
       "        [ 0.87599428,  0.87563151,  0.99941941,  0.8873018 ,  0.92442958],\n",
       "        [ 0.88416403,  0.88380059,  0.99962594,  0.89690621,  0.93408454],\n",
       "        [ 0.88327991,  0.88291646,  0.99960722,  0.89587593,  0.93307116],\n",
       "        [ 0.87117076,  0.8708092 ,  0.99925602,  0.88154589,  0.9184254 ],\n",
       "        [ 0.8752916 ,  0.87492897,  0.99939774,  0.88646713,  0.92356886],\n",
       "        [ 0.87954233,  0.87917906,  0.99951879,  0.89149581,  0.92870257],\n",
       "        [ 0.89910674,  0.89874689,  0.99984477,  0.91396147,  0.95004362],\n",
       "        [ 0.87017625,  0.86981501,  0.99921777,  0.88035158,  0.91715969],\n",
       "        [ 0.87411288,  0.87375052,  0.99935983,  0.88506404,  0.92211432],\n",
       "        [ 0.87302687,  0.87266477,  0.99932308,  0.88376804,  0.92076233],\n",
       "        [ 0.87573915,  0.87537644,  0.99941162,  0.8869989 ,  0.92411763],\n",
       "        [ 0.89432632,  0.89396461,  0.99979188,  0.90858118,  0.94517911],\n",
       "        [ 0.87257551,  0.87221354,  0.99930727,  0.88322849,  0.92019708],\n",
       "        [ 0.87316306,  0.87280093,  0.99932778,  0.88393073,  0.92093249],\n",
       "        [ 0.87442814,  0.8740657 ,  0.99937016,  0.88543967,  0.92250466],\n",
       "        [ 0.87785148,  0.87748841,  0.99947344,  0.88950142,  0.92668149],\n",
       "        [ 0.87661602,  0.87625315,  0.99943801,  0.88803922,  0.92518719],\n",
       "        [ 0.88482329,  0.88445988,  0.9996394 ,  0.89767299,  0.93483519],\n",
       "        [ 0.87117076,  0.8708092 ,  0.99925602,  0.88154589,  0.9184254 ],\n",
       "        [ 0.88482329,  0.88445988,  0.9996394 ,  0.89767299,  0.93483519],\n",
       "        [ 0.89268899,  0.89232679,  0.99977052,  0.90672153,  0.94346067],\n",
       "        [ 0.87080567,  0.87044422,  0.99924217,  0.88110775,  0.91796185],\n",
       "        [ 0.87515828,  0.87479568,  0.99939355,  0.88630862,  0.92340501],\n",
       "        [ 0.86683899,  0.86647898,  0.99907666,  0.8763255 ,  0.91284348],\n",
       "        [ 0.87519094,  0.87482833,  0.99939458,  0.88634745,  0.92344517],\n",
       "        [ 0.87131395,  0.87095234,  0.99926139,  0.88171763,  0.91860685],\n",
       "        [ 0.86930233,  0.86894138,  0.99918277,  0.8793    ,  0.91603965],\n",
       "        [ 0.88678153,  0.88641825,  0.99967694,  0.89994309,  0.93703969],\n",
       "        [ 0.89432632,  0.89396461,  0.99979188,  0.90858118,  0.94517911],\n",
       "        [ 0.86290584,  0.86254763,  0.99888235,  0.87154511,  0.90762133],\n",
       "        [ 0.88327991,  0.88291646,  0.99960722,  0.89587593,  0.93307116],\n",
       "        [ 0.86760455,  0.86724424,  0.99911085,  0.87725155,  0.91384297],\n",
       "        [ 0.87766684,  0.8773038 ,  0.99946827,  0.88928317,  0.92645911],\n",
       "        [ 0.88764249,  0.88727931,  0.99969234,  0.90093757,  0.93799698],\n",
       "        [ 0.87415404,  0.87379167,  0.99936118,  0.8851131 ,  0.92216534],\n",
       "        [ 0.87302687,  0.87266477,  0.99932308,  0.88376804,  0.92076233],\n",
       "        [ 0.8730439 ,  0.8726818 ,  0.99932367,  0.88378839,  0.92078361],\n",
       "        [ 0.87440075,  0.87403832,  0.99936927,  0.88540705,  0.92247079],\n",
       "        [ 0.86172121,  0.8613636 ,  0.99881725,  0.87009796,  0.90602   ],\n",
       "        [ 0.87945684,  0.87909358,  0.99951658,  0.89139516,  0.92860105],\n",
       "        [ 0.88678153,  0.88641825,  0.99967694,  0.89994309,  0.93703969],\n",
       "        [ 0.89186198,  0.89149957,  0.99975903,  0.90577904,  0.94258258],\n",
       "        [ 0.87723509,  0.87687211,  0.99945601,  0.88877243,  0.92593781],\n",
       "        [ 0.89674101,  0.89638015,  0.99982026,  0.91130814,  0.94766473],\n",
       "        [ 0.87048984,  0.87012849,  0.99923001,  0.88072845,  0.91755981],\n",
       "        [ 0.87704192,  0.87667897,  0.99945045,  0.88854375,  0.92570399],\n",
       "        [ 0.87701872,  0.87665578,  0.99944978,  0.88851628,  0.92567588],\n",
       "        [ 0.87038466,  0.87002334,  0.99922593,  0.88060207,  0.91742571],\n",
       "        [ 0.87072442,  0.870363  ,  0.99923906,  0.8810102 ,  0.91785851],\n",
       "        [ 0.89186198,  0.89149957,  0.99975903,  0.90577904,  0.94258258],\n",
       "        [ 0.86940005,  0.86903907,  0.99918675,  0.87941768,  0.91616526],\n",
       "        [ 0.87994463,  0.87958132,  0.99952907,  0.89196916,  0.92917934],\n",
       "        [ 0.89910674,  0.89874689,  0.99984477,  0.91396147,  0.95004362],\n",
       "        [ 0.86413457,  0.86377576,  0.99894655,  0.87304256,  0.90926838],\n",
       "        [ 0.88807083,  0.8877077 ,  0.99969977,  0.90143151,  0.93847051],\n",
       "        [ 0.87411288,  0.87375052,  0.99935983,  0.88506404,  0.92211432],\n",
       "        [ 0.84313344,  0.8427893 ,  0.99726381,  0.84697806,  0.8792131 ],\n",
       "        [ 0.87458551,  0.87422304,  0.99937527,  0.88562708,  0.92269915],\n",
       "        [ 0.87128077,  0.87091917,  0.99926015,  0.88167784,  0.91856482],\n",
       "        [ 0.87199385,  0.87163204,  0.99928644,  0.8825324 ,  0.91946578],\n",
       "        [ 0.89910674,  0.89874689,  0.99984477,  0.91396147,  0.95004362],\n",
       "        [ 0.88203231,  0.88166888,  0.99957941,  0.89441826,  0.93162812],\n",
       "        [ 0.88194276,  0.88157933,  0.99957735,  0.89431345,  0.93152395],\n",
       "        [ 0.87675695,  0.87639406,  0.99944215,  0.88820623,  0.9253584 ],\n",
       "        [ 0.88807083,  0.8877077 ,  0.99969977,  0.90143151,  0.93847051],\n",
       "        [ 0.86862879,  0.86826809,  0.99915486,  0.87848822,  0.91517147],\n",
       "        [ 0.87599428,  0.87563151,  0.99941941,  0.8873018 ,  0.92442958],\n",
       "        [ 0.88934728,  0.88898435,  0.99972099,  0.90290018,  0.93987089],\n",
       "        [ 0.88232124,  0.8819578 ,  0.999586  ,  0.89475623,  0.93196366],\n",
       "        [ 0.8814941 ,  0.88113069,  0.9995669 ,  0.89378805,  0.93100089],\n",
       "        [ 0.86332441,  0.86296599,  0.99890459,  0.87205563,  0.908184  ]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.66339757,  0.7132823 ,  0.97382122,  0.73408313,  0.8735975 ]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict to find error\n",
    "network.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Cross Entropy Loss Function')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs,epochsMeanError)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title(\"Cross Entropy Loss Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Squared Error Loss Function')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuYXFWd7vHv29Xd6VIgQNJqTMgEJajBOHFsIt44DirGOUg4Myi3UVDO5IxHjs7jZcQbKsIcmTNH5/GRccQBAeUigzLkaDQyIjoqQhqNhIAMDSI0iRIIl0BudOd3/tirwu5KVXVVd+/u0PV+nmc/2bX23qvW6u7Ur9Zae+2liMDMzGysOqa6AGZm9szmQGJmZuPiQGJmZuPiQGJmZuPiQGJmZuPiQGJmZuPiQGJtQ1JIOmSqy2G1STpF0g+muhzWOgcSa4mk10r6uaTHJG2W9DNJh091ucZL0g2Stkt6Irf9v0kuw4IU7Don833Te18saWdV/U8o8P32qGtEXBYRRxf1nlacSf+DtWcuSfsB3wHeA1wFdAOvA3ZMQVlKETE8wdmeERH/0sR7d0bE0GhpreaxF/j7iPjEVBfCnnncIrFWHAoQEVdExHBEbIuIH0TErZB9uEv6B0kPSbpH0nvz3zol3SvpjZXMJH1a0jdyr/9V0u9Ta+cnkg7LHbtY0pclrZL0JPCnkmak97tP0h8k/bOkcu6aD0vaKGmDpHePtdKSXi9pUNJHJP0e+FqttHTuX0kaSK21lZKen8sn0s/kLuCuFsswQ9I/prpsSPsz0rHZkr4j6dH0vv8hqSMd+4ikByRtkXSnpDeMof4jugTT7+Kcqp/NByU9mH7e78qdW5b0fyX9Lv1ef5p+Rz9JpzyaWj+vknSapJ/mrn21pDXpujWSXp07doOkz6YW8RZJP5A0u9W62cRwILFW/CcwLOkSSW+RdEDV8b8CjgFeDvQBx7eY//eAhcBzgF8Cl1UdPxk4F9gX+ClwHllwWwIcAswFzgKQtAz4EPCmlOcbGZ/nAQcCfwSsqJUm6SjgfwNvB+YAvwOurMrnOOCVwKIW3//jwBFkdf1jYClQaT18EBgEeoHnAh8DQtKLgDOAwyNiX+DNwL0tvm8zngfMJPv5nw6cn/vb+AfgFcCryX5WfwvsAo5Mx/ePiH0i4sZ8hpIOBL4LfBGYBXwe+K6kWbnTTgbeRfb30k32+7apEBHevDW9AS8BLib74BoCVgLPTceuB/46d+7RQACd6fW9wBtzxz8NfKPO++yfrp2ZXl8MXJo7LuBJ4IW5tFcBv037FwGfyx07NOV3SJ33uwHYCjya2z6bjr0e2An05M6vlXYhWfdQ5fU+wFPAgvQ6gKMa/GwX5H9eVcfuBv4s9/rNwL1p/2zg2uq6kQXXB8mCaNcov9eLge25uj+UOzbi55bOPSf3c9iWL3N6zyPIvqhuA/64mboCpwE/TfvvAG6uuuZG4LTc7+sTuWP/E/j+VP//aNfNLRJrSUTcERGnRcQ84KXA84F/TIefD9yfO/13zeabusU+J+luSY/z9DfnfHdFPu9e4FnALalL51Hg+yl9rGV5X0Tsn9s+mTu2KSK2V51fnfb8/PtExBPAw2Tf1GvVoRUj8k77lW6z/wMMAD9IXYpnpvcfAP6GLGA/KOnKfFdbDf+Qq3sr3UQPx8jxnq1kQXQ20EMWBFtVXV/S6/zP8vc13tOmgAOJjVlE/Ibs2+lLU9JG4KDcKfOrLnmS7MO/4nm5/ZOB5WTfnmeSfWOFrOWx+y1z+w+Rfds9LPfhNzMiKh8mo5WlVbUek12dtoGsmwsASc8m65Z5YJR8mjEib7L6bACIiC0R8cGIeAHwVuADlbGQiLg8Il6brg2y7sBWbaX+762Rh8haOS+scWy0n0N1fSGr8wM1zrUp5kBiTZP04jSoOi+9Pgg4CfhFOuUq4H2S5qU+8jOrslgLnCipS1L1GMq+ZHd/PUz2ofV3jcoSEbuArwJfkPScVJ65kt6cK8tpkhZJehbwqbHVuiWXA++StCQNhP8dcFNE3NtiPjMk9eS2DuAK4BOSetOg8lnANwAkHSPpEEkCHgeGycayXiTpqFSW7WSBdyx3uq0FTk6txmXAf2nmovQ7ugj4vKTnp+tflcqziWys5AV1Ll8FHCrpZEmdym5FXkR216DtZRxIrBVbyAaKb1J259QvgNvIBnsh+2BfDfyabLD821XXf5Ls2+kjwGfIPngrLiXrungAuJ2ng1MjHyHr0vlF6g77d+BFABHxPbIut+vTOdc3kd+XNHIexS1NXLNbRPyQrI7fImsRvRA4sZU8kifIPvQr21HAOUA/cCuwjuzne046fyFZ3Z8gG0f4p4i4AZgBfI6sZfB7skHpj42hPO8na+k8CpwC/FsL134olXcNsJmsRdQREVvJbpz4WeqaPCJ/UUQ8THbjxgfJvlz8LXBMRDw0hvJbwRThha2sGJIWAL8lG+jd2+ZMmNkEcYvEzMzGxYHEzMzGxV1bZmY2Lm6RmJnZuLTFQxtnz54dCxYsmOpimJk9o9xyyy0PRUTvaOe1RSBZsGAB/f39U10MM7NnFElNPZ3CXVtmZjYuDiRmZjYuDiRmZjYuDiRmZjYuhQYSScvSqmwDlUdbVx3/gKTbJd0q6YeS8k9OPVXSXWk7NZf+CknrUp5fTA+qMzOzKVJYIJFUAs4H3kL21M6TJFWvCvcroC8iXgZcDfx9uvZAsqe1vpJsJbhP5VZc+zLZCnUL07asqDqYmdnoimyRLAUGIuKeiNhJtuTo8vwJEfGj9BRQyJ72Oi/tvxm4LiI2R8QjwHXAMklzgP0i4sbIpuRfSrZ0qZmZTZEiA8lcRq4GN8jI1c2qnU62Zneja+em/VHzlLRCUr+k/k2bNrVY9Mw1vxrkG79oepE/M7O2VGQgqTV2UfPBXpL+EugjWzK00bVN5xkRF0REX0T09faOOjGzpu/8eiNXrrlvTNeambWLIgPJICOXOp1HWho0T9IbgY8Dx0bEjlGuHeTp7q+6eU6Unu4S23aOZUE5M7P2UWQgWQMslHSwpG6yleJW5k+Q9HLgK2RB5MHcodXA0ZIOSIPsRwOrI2IjsEXSEelurXcC1xZVgXJXie1P7SoqezOzaaGwZ21FxJCkM8iCQgm4KCLWSzob6I+IlWRdWfsA/5ru4r0vIo6NiM2SPksWjADOjojNaf89wMVAmWxM5XsUJAskbpGYmTVS6EMbI2IVsKoq7azc/hsbXHsRcFGN9H7gpRNYzLrK3SW2OZCYmTXkme0N9HRlgcSLf5mZ1edA0kBPVwcRsGPI4yRmZvU4kDRQ7ioBeJzEzKwBB5IGKoHE4yRmZvU5kDRQ7k6BxHNJzMzqciBpoMctEjOzUTmQNOAxEjOz0TmQNPB015bv2jIzq8eBpAG3SMzMRudA0oDHSMzMRudA0kBPV/bjcSAxM6vPgaQBd22ZmY3OgaQBzyMxMxudA0kDPZ0eIzEzG40DSQMdHWJGZ4cDiZlZAw4koyh3l9juri0zs7oKDSSSlkm6U9KApDNrHD9S0i8lDUk6Ppf+p5LW5rbtko5Lxy6W9NvcsSVF1qHc5cWtzMwaKWyFREkl4HzgTcAgsEbSyoi4PXfafcBpwIfy10bEj4AlKZ8DgQHgB7lTPhwRVxdV9jyv225m1liRS+0uBQYi4h4ASVcCy4HdgSQi7k3HGn1SHw98LyK2FlfU+nrcIjEza6jIrq25wP2514MprVUnAldUpZ0r6VZJX5A0Y6wFbEZPV4fnkZiZNVBkIFGNtJYWP5c0B1gMrM4lfxR4MXA4cCDwkTrXrpDUL6l/06ZNrbztCOXukueRmJk1UGQgGQQOyr2eB2xoMY+3A9dExFOVhIjYGJkdwNfIutD2EBEXRERfRPT19va2+LZP82C7mVljRQaSNcBCSQdL6ibrolrZYh4nUdWtlVopSBJwHHDbBJS1Lo+RmJk1VlggiYgh4Ayybqk7gKsiYr2ksyUdCyDpcEmDwNuAr0haX7le0gKyFs2Pq7K+TNI6YB0wGzinqDpAumvLXVtmZnUVedcWEbEKWFWVdlZufw1Zl1eta++lxuB8RBw1saVsrNztFomZWSOe2T4Kj5GYmTXmQDKKnjQhMaKlG87MzNqGA8koKo+S3zHk2e1mZrU4kIyipzOtkugBdzOzmhxIRrF7cSuPk5iZ1eRAMoqeLgcSM7NGHEhGUVm33V1bZma1OZCMotK15Qc3mpnV5kAyirK7tszMGnIgGUWPu7bMzBpyIBmF79oyM2vMgWQUla6tHV5u18ysJgeSUfj2XzOzxhxIRuHBdjOzxhxIRjHDj0gxM2vIgWQUHR2ip6vD80jMzOpwIGmC1yQxM6uv0EAiaZmkOyUNSDqzxvEjJf1S0pCk46uODUtam7aVufSDJd0k6S5J30zrwReq3FVy15aZWR2FBRJJJeB84C3AIuAkSYuqTrsPOA24vEYW2yJiSdqOzaWfB3whIhYCjwCnT3jhq/R4uV0zs7qKbJEsBQYi4p6I2AlcCSzPnxAR90bErUBTkzQkCTgKuDolXQIcN3FFrq3cVfIYiZlZHUUGkrnA/bnXgymtWT2S+iX9QlIlWMwCHo2IodHylLQiXd+/adOmVss+Qjktt2tmZnvqLDBv1UhrZeHz+RGxQdILgOslrQMebzbPiLgAuACgr69vXAuu93iw3cysriJbJIPAQbnX84ANzV4cERvSv/cANwAvBx4C9pdUCYAt5TlWPR5sNzOrq8hAsgZYmO6y6gZOBFaOcg0Akg6QNCPtzwZeA9weEQH8CKjc4XUqcO2El7xKudtjJGZm9RQWSNI4xhnAauAO4KqIWC/pbEnHAkg6XNIg8DbgK5LWp8tfAvRL+jVZ4PhcRNyejn0E+ICkAbIxkwuLqkNFuavDXVtmZnUUOUZCRKwCVlWlnZXbX0PWPVV93c+BxXXyvIfsjrBJ4wmJZmb1eWZ7E3q6PUZiZlaPA0kTyl0ldgztYteucd38ZWY2LTmQNKHyKPntQ26VmJlVcyBpQmW5XU9KNDPbkwNJE3o6vbiVmVk9DiRN6EktEg+4m5ntyYGkCbvHSNwiMTPbgwNJE7xuu5lZfQ4kTSh3e912M7N6HEia0OMWiZlZXQ4kTfAYiZlZfQ4kTSj7ri0zs7ocSJrgwXYzs/ocSJrQ0+WZ7WZm9TiQNGFGZ7pryy0SM7M9OJA0QRLlLq+SaGZWiwNJk8pek8TMrKaGgURSh6Tbxpq5pGWS7pQ0IOnMGsePlPRLSUOSjs+lL5F0o6T1km6VdELu2MWSfitpbdqWjLV8rfAqiWZmtTVcajcidkn6taT5EXFfKxlLKgHnA28CBoE1klbm1l4HuA84DfhQ1eVbgXdGxF2Sng/cIml1RDyajn84Iq5upTzj1eN1283MampmzfY5wHpJNwNPVhIj4thRrlsKDKQ11pF0JbAc2B1IIuLedGzE7VAR8Z+5/Q2SHgR6gUeZIuXuEtvdtWVmtodmAslnxpj3XOD+3OtB4JWtZiJpKdAN3J1LPlfSWcAPgTMjYkeN61YAKwDmz5/f6tvuwV1bZma1jTrYHhE/Bn4D7Ju2O1LaaFQru1YKJ2kO8HXgXRFRabV8FHgxcDhwIPCROuW+ICL6IqKvt7e3lbetqceBxMysplEDiaS3AzcDbwPeDtyUHxhvYBA4KPd6HrCh2YJJ2g/4LvCJiPhFJT0iNkZmB/A1si60wvV0lTwh0cyshma6tj4OHB4RDwJI6gX+HRhtsHsNsFDSwcADwInAyc0USlI3cA1waUT8a9WxORGxUZKA44Ax31XWCs8jMTOrrZl5JB2VIJI83Mx1ETEEnAGsBu4AroqI9ZLOlnQsgKTDJQ2StXa+Iml9uvztwJHAaTVu871M0jpgHTAbOKeJOoxbucvzSMzMammmRfJ9SauBK9LrE4BVzWQeEauqz42Is3L7a8i6vKqv+wbwjTp5HtXMe0+0crfHSMzMahk1kETEhyX9OfBasgH0CyLimsJLtpfxYLuZWW0NA0maVLg6It4IfHtyirR3KneV2Dm0i+FdQamj1g1pZmbtqeFYR0QMA1slzZyk8uy1Kuu2e8DdzGykZsZItgPrJF3HyJnt7yusVHuh/OJWz57RzI/NzKw9NPOJ+N20tbXK4la+c8vMbKRmxkjeFBF/OUnl2WtVAsmOIQcSM7O8ZsZIetMEwba2u2trp2e3m5nlNdO1dS/wM0krGTlG8vmiCrU3Knc/PUZiZmZPayaQbEhbB9lDG9tST5cDiZlZLc1MSNzjMfKS2u62pbIH283Maqo7RiLpp7n9r1cdvrmwEu2lKl1bnkdiZjZSo8H2Z+f2X1p1rO2mdpfdtWVmVlOjQBJ19mu9nvbctWVmVlujsY79Jf03smCzf3pwI2StkbZ7ZEpPekSKWyRmZiM1CiQ/Bo7N7b81d+wnhZVoL9Vd6kDyGImZWbW6gSQi3jWZBdnbSfIqiWZmNTSzQuKYSVom6U5JA5LOrHH8SEm/lDRUvQ68pFMl3ZW2U3Ppr5C0LuX5xbTk7qQoe00SM7M9FBZI0nO6zgfeAiwCTpK0qOq0+4DTgMurrj0Q+BTwSmAp8ClJB6TDXwZWAAvTtqygKuyhp6vkR6SYmVUpskWyFBiIiHsiYidwJbA8f0JE3BsRtwLVn85vBq6LiM0R8QhwHbBM0hxgv4i4MSICuBQ4rsA6jFDudteWmVm1pmaoS3o1sCB/fkRcOsplc4H7c68HyVoYzah17dy0DdZIr1XmFWQtF+bPn9/k2zbmri0zsz2NGkjSrPYXAmuByqdopTXQ8NIaac3OP6l3bdN5RsQFwAUAfX19EzLvpdxV8jwSM7MqzbRI+oBFqSupFYPAQbnX88ge/tjsta+vuvaGlD5vjHmOW093ice3PTVZb2dm9ozQzBjJbcDzxpD3GmChpIPTeiYnAiubvHY1cLSkA9Ig+9HA6ojYCGyRdES6W+udwLVjKNuYlLs6PEZiZlalmRbJbOB2STcDOyqJEXFs/UsgIoYknUEWFErARRGxXtLZQH9ErJR0OHANcADwVkmfiYjDImKzpM+SBSOAsyNic9p/D3AxUAa+l7ZJ4TESM7M9NRNIPj3WzCNiFbCqKu2s3P4aRnZV5c+7CLioRno/ez5EclL0eEKimdkemlmP5MeTUZBngh4PtpuZ7WHUMZI0HrFG0hOSdkoalvT4ZBRub5PNI/GERDOzvGYG278EnATcRTYu8d9TWtspd5XYObyLoWEHEzOziqZmtkfEAFCKiOGI+Bojb81tG5U1SbYPOZCYmVU0M9i+Nd2+u1bS3wMbGbl6Ytvo6X56cat9ZrTdsvVmZjU10yJ5RzrvDOBJskmGf1FkofZWu1skvnPLzGy3Zu7a+p2kMjAnIj4zCWXaa3nddjOzPTVz19ZbyZ6z9f30eomkZmeoTyvlynK7vgXYzGy3Zrq2Pk32SPhHASJiLdmTgNtOT6e7tszMqjUTSIYi4rHCS/IMsHuw3YHEzGy3Zm49uk3SyUBJ0kLgfcDPiy3W3smD7WZme2qmRfK/gMPIHth4BfA48DdFFmpv5cF2M7M9NXPX1lbg42lra+Xd80g8IdHMrKJuIBntzqzRHiM/HfW4RWJmtodGLZJXka2bfgVwE7WXuW0rHiMxM9tTo0DyPOBNZA9sPBn4LnBFRKyfjILtjbpKotQhzyMxM8upO9ieHtD4/Yg4FTgCGABukPS/Jq10exlJXiXRzKxKw7u2JM2Q9OfAN4D3Al8Evt1s5pKWSbpT0oCkM+vk/810/CZJC1L6KZLW5rZdkpakYzekPCvHntN8dcevp6vDgcTMLKfRYPslZEvafg/4TETc1krGkkrA+WTdY4PAGkkrI+L23GmnA49ExCGSTgTOA06IiMuAy1I+i4Fr04z6ilPSkruTzsvtmpmN1KhF8g7gUOD9wM8lPZ62LU2ukLgUGIiIeyJiJ3AlsLzqnOXAJWn/auANkqoH9U8iG/DfK5QdSMzMRqjbIomIpha9amAu2V1fFYPAK+udExFDkh4DZgEP5c45gT0D0NckDQPfAs6JiKh+c0krgBUA8+fPH0c1Rip3e912M7O88QaLRmrdLlz9gd/wHEmvBLZWdaudEhGLgdel7R213jwiLoiIvojo6+3tba3kDfR4sN3MbIQiA8kg2SJYFfOADfXOkdQJzAQ2546fSFW3VkQ8kP7dAlxO1oU2abK7tjyz3cysoshAsgZYKOngtFTviUD1bPmVwKlp/3jg+ko3laQO4G1kYyuktE5Js9N+F3AM0NJNAONV7iqx3V1bZma7FbbweBrzOANYDZSAiyJivaSzgf6IWAlcCHxd0gBZS+TEXBZHAoMRcU8ubQawOgWREvDvwFeLqkMt5W53bZmZ5RUWSAAiYhWwqirtrNz+drJWR61rbyCbCJlPexJ4xYQXtAUeIzEzG6nIrq1pqaerw11bZmY5DiQtKneV2D7kQGJmVuFA0qJyV4mnhoOnhn3nlpkZOJC0rLK4lWe3m5llHEha5MWtzMxGciBp0e7FrbzcrpkZ4EDSst3rtrtFYmYGOJC0rOyuLTOzERxIWrR7jMRzSczMAAeSlvV0ZT8y37VlZpZxIGmRx0jMzEZyIGnR7ru2HEjMzAAHkpZ5sN3MbCQHkhb1dHuw3cwsz4GkRe7aMjMbyYGkRV2lDjo75K4tM7Ok0EAiaZmkOyUNSDqzxvEZkr6Zjt8kaUFKXyBpm6S1afvn3DWvkLQuXfNFSSqyDrWUu0ps8yNSzMyAAgOJpBJwPvAWYBFwkqRFVaedDjwSEYcAXwDOyx27OyKWpO2vc+lfBlYAC9O2rKg61NPj5XbNzHYrskWyFBiIiHsiYidwJbC86pzlwCVp/2rgDY1aGJLmAPtFxI0REcClwHETX/TGero6PEZiZpYUGUjmAvfnXg+mtJrnRMQQ8BgwKx07WNKvJP1Y0uty5w+OkicAklZI6pfUv2nTpvHVpErWteVAYmYGxQaSWi2LaPKcjcD8iHg58AHgckn7NZlnlhhxQUT0RURfb29vC8UenZfbNTN7WpGBZBA4KPd6HrCh3jmSOoGZwOaI2BERDwNExC3A3cCh6fx5o+RZuB63SMzMdisykKwBFko6WFI3cCKwsuqclcCpaf944PqICEm9abAeSS8gG1S/JyI2AlskHZHGUt4JXFtgHWoqd5c8RmJmlnQWlXFEDEk6A1gNlICLImK9pLOB/ohYCVwIfF3SALCZLNgAHAmcLWkIGAb+OiI2p2PvAS4GysD30japyl0lNjiQmJkBBQYSgIhYBayqSjsrt78deFuN674FfKtOnv3ASye2pK0pd/n2XzOzCs9sH4Oebk9INDOrcCAZg3KXx0jMzCocSMagp6uDbU8Nk82JNDNrbw4kYzBnZpnhXcEDj26b6qKYmU05B5IxWDx3JgDrBh+b4pKYmU09B5IxePGcfekqiVsfcCAxM3MgGYMZnSVe9Lx9uc2BxMzMgWSsFs+dya2Dj3nA3czangPJGC2euz+PbXuK+zd7wN3M2psDyRi9bF4acHf3lpm1OQeSMTr0ufvSXerg1gceneqimJlNKQeSMeru7ODFc/b1LcBm1vYcSMZh8dyZrHvAA+5m1t4cSMZh8dyZbNk+xO8e3jrVRTEzmzIOJOOwOA24e2KimbUzB5JxOPS5+9Ld2eGJiWbW1hxIxqGr1MFL5uzHrYO+c8vM2lehgUTSMkl3ShqQdGaN4zMkfTMdv0nSgpT+Jkm3SFqX/j0qd80NKc+1aXtOkXUYzcvmzuS2Bx5n1y4PuJtZeyoskEgqAecDbwEWASdJWlR12unAIxFxCPAF4LyU/hDw1ohYDJwKfL3qulMiYknaHiyqDs1YPG8mT+wY4rcPPzmVxTAzmzJFtkiWAgMRcU9E7ASuBJZXnbMcuCTtXw28QZIi4lcRsSGlrwd6JM0osKxjVnmkvMdJzKxdFRlI5gL3514PprSa50TEEPAYMKvqnL8AfhURO3JpX0vdWp+UpFpvLmmFpH5J/Zs2bRpPPRpa+Jx9mNHZwa2emGhmbarIQFLrA756IKHhOZIOI+vu+h+546ekLq/Xpe0dtd48Ii6IiL6I6Ovt7W2p4K3oLHVw2PP38wx3M2tbRQaSQeCg3Ot5wIZ650jqBGYCm9PrecA1wDsj4u7KBRHxQPp3C3A5WRfalFo8dybrNzzGsAfczawNFRlI1gALJR0sqRs4EVhZdc5KssF0gOOB6yMiJO0PfBf4aET8rHKypE5Js9N+F3AMcFuBdWjK4nn78+TOYX770BNTXRQzs0lXWCBJYx5nAKuBO4CrImK9pLMlHZtOuxCYJWkA+ABQuUX4DOAQ4JNVt/nOAFZLuhVYCzwAfLWoOjSr8kh5j5OYWTvqLDLziFgFrKpKOyu3vx14W43rzgHOqZPtKyayjBPhhb37UO4qse6Bx/jzP5k31cUxM5tUntk+AUod8oC7mbUtB5IJsnjeTNZveNwD7mbWdhxIJsjL5s1k21PD3L3JA+5m1l4cSCZIZYa7B9zNrN04kEyQg2fvw7O7S6zzk4DNrM04kEyQUoc4bO5ML3JlZm3HgWQCLZ47k9s3PM7Q8K6pLoqZ2aRxIJlAL5s3kx1Du7iqf3Cqi2JmNmkcSCbQ0Yuex6tfOIuPXbOO877/Gy92ZWZtwYFkApW7S1zy7qWc/Mr5fPmGu3nPZbewdefQVBfLzKxQDiQTrKvUwbnHvZSzjlnEdbf/geO/fCMbH9s21cUyMyuMA0kBJPHu1x7Mhacdzn2bt3Lsl37G2vt9W7CZTU+KmP79+H19fdHf3z8l7/2ff9jCuy9ew6YtO3jdwtkcdOCzOOiAZzH/wGdl+weWeVZ3oc/ONDMbE0m3RETfaOf5E6xghz53X65972s497t3cPvGx7nx7od5cufwiHO6Sx10lkRnh+ju7KCzI3td6hAia+FAWk4yrSlZc33h3LlmZgAXnXo482c9q9D3cCCZBLP2mcHnT1gCQESw+cmd3P/INu7fvJX7Nm9ly/YhhoZ3MbQr2Dm8K9sfDoYjqDQYI11b2a9p+jcuzdpWEKjuV8j6ujuLH8FwIJlkkpi1zwxm7TODJQd1YErWAAAGK0lEQVTtP9XFMTMbt0JDlaRlku6UNCDpzBrHZ0j6Zjp+k6QFuWMfTel3Snpzs3mamdnkKiyQSCoB5wNvARYBJ0laVHXa6cAjEXEI8AXgvHTtIrI13g8DlgH/JKnUZJ5mZjaJimyRLAUGIuKeiNgJXAksrzpnOXBJ2r8aeIOy0eLlwJURsSMifgsMpPyaydPMzCZRkYFkLnB/7vVgSqt5TkQMAY8Bsxpc20yeAEhaIalfUv+mTZvGUQ0zM2ukyEBS6/aC6vuK6p3TavqeiREXRERfRPT19vY2LKiZmY1dkYFkEDgo93oesKHeOZI6gZnA5gbXNpOnmZlNoiIDyRpgoaSDJXWTDZ6vrDpnJXBq2j8euD6yyRIrgRPTXV0HAwuBm5vM08zMJlFh80giYkjSGcBqoARcFBHrJZ0N9EfESuBC4OuSBshaIiema9dLugq4HRgC3hsRwwC18iyqDmZmNrq2eNaWpE3A78Z4+WzgoQkszjOF691e2rXe0L51b6befxQRow4yt0UgGQ9J/c08tGy6cb3bS7vWG9q37hNZbz9G3szMxsWBxMzMxsWBZHQXTHUBpojr3V7atd7QvnWfsHp7jMTMzMbFLRIzMxsXBxIzMxsXB5IG2mXtE0kXSXpQ0m25tAMlXSfprvTvAVNZxiJIOkjSjyTdIWm9pPen9Gldd0k9km6W9OtU78+k9IPTukB3pXWCuqe6rEVIS1L8StJ30utpX29J90paJ2mtpP6UNmF/5w4kdbTZ2icXk637kncm8MOIWAj8ML2eboaAD0bES4AjgPem3/F0r/sO4KiI+GNgCbBM0hFk6wF9IdX7EbL1gqaj9wN35F63S73/NCKW5OaOTNjfuQNJfW2z9klE/ITsETV5+bViLgGOm9RCTYKI2BgRv0z7W8g+XOYyzesemSfSy660BXAU2bpAMA3rDSBpHvBfgX9Jr0Ub1LuOCfs7dyCpr+m1T6ap50bERsg+cIHnTHF5CpWWeX45cBNtUPfUvbMWeBC4DrgbeDStCwTT9+/9H4G/BXal17Noj3oH8ANJt0hakdIm7O+8sIc2TgNNr31iz2yS9gG+BfxNRDyefUmd3tJDUJdI2h+4BnhJrdMmt1TFknQM8GBE3CLp9ZXkGqdOq3onr4mIDZKeA1wn6TcTmblbJPW1+9onf5A0ByD9++AUl6cQkrrIgshlEfHtlNwWdQeIiEeBG8jGiPZP6wLB9Px7fw1wrKR7ybqqjyJroUz3ehMRG9K/D5J9cVjKBP6dO5DU1+5rn+TXijkVuHYKy1KI1D9+IXBHRHw+d2ha111Sb2qJIKkMvJFsfOhHZOsCwTSsd0R8NCLmRcQCsv/P10fEKUzzekt6tqR9K/vA0cBtTODfuWe2NyDpz8i+sVTWPjl3iotUCElXAK8ne6z0H4BPAf8GXAXMB+4D3hYR1QPyz2iSXgv8B7COp/vMP0Y2TjJt6y7pZWSDqyWyL5NXRcTZkl5A9k39QOBXwF9GxI6pK2lxUtfWhyLimOle71S/a9LLTuDyiDhX0iwm6O/cgcTMzMbFXVtmZjYuDiRmZjYuDiRmZjYuDiRmZjYuDiRmZjYuDiRmE0DScHqyamWbsAc9SlqQfzKz2d7Gj0gxmxjbImLJVBfCbCq4RWJWoLQOxHlp/Y+bJR2S0v9I0g8l3Zr+nZ/SnyvpmrRWyK8lvTplVZL01bR+yA/SjHSzvYIDidnEKFd1bZ2QO/Z4RCwFvkT2pATS/qUR8TLgMuCLKf2LwI/TWiF/AqxP6QuB8yPiMOBR4C8Kro9Z0zyz3WwCSHoiIvapkX4v2SJS96QHRP4+ImZJegiYExFPpfSNETFb0iZgXv4RHekR99elBYiQ9BGgKyLOKb5mZqNzi8SseFFnv945teSf/TSMxzdtL+JAYla8E3L/3pj2f072BFqAU4Cfpv0fAu+B3YtP7TdZhTQbK3+rMZsY5bTiYMX3I6JyC/AMSTeRfXE7KaW9D7hI0oeBTcC7Uvr7gQsknU7W8ngPsLHw0puNg8dIzAqUxkj6IuKhqS6LWVHctWVmZuPiFomZmY2LWyRmZjYuDiRmZjYuDiRmZjYuDiRmZjYuDiRmZjYu/x9tu7BKJN+Q+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs,epochsMeanError)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title(\"Squared Error Loss Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
