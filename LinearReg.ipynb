{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "TARGET_VARIABLES = ['appearance', 'aroma', 'overall', 'palate',\n",
    "       'taste']\n",
    "def pad_ones_column(X):\n",
    "    \"\"\"Add columns of ones to a data matrix\"\"\"\n",
    "    return np.c_[ np.ones(len(ORIGINAL_DF)), X ]\n",
    "\n",
    "def estimate_beta(X, y):\n",
    "    \"\"\"Compte estimates for beta given X and y\n",
    "    \n",
    "    Beta = (X^T X)^-1 X^T y\n",
    "    \"\"\"\n",
    "    t1 = np.linalg.inv(np.matmul(X.T, X))\n",
    "    t2 = np.matmul(X.T, y)\n",
    "    return np.matmul(t1, t2)\n",
    "\n",
    "def run_complete_linear_regression(X, y, beta_hat):\n",
    "    \"\"\"Run linear regression and produce error statistics\"\"\"\n",
    "    # Make predictions based on testing data\n",
    "    predicted_y = np.matmul(X, beta_hat)\n",
    "    \n",
    "    # Compute the error from the actual data\n",
    "    errors = np.square(y - predicted_y)\n",
    "    \n",
    "    # Compute mean squared error\n",
    "    MSE = np.mean(errors)\n",
    "    \n",
    "    # Compute sum squared error\n",
    "    SSE = np.sum(errors)\n",
    "\n",
    "    return MSE, SSE\n",
    "\n",
    "def forward_stepwise_selection(df, target, all_variables, to_minimize, max_vars, eps, **kwargs):\n",
    "    available_vars = set(all_variables)\n",
    "    min_vars = len(available_vars) - max_vars\n",
    "    current_vars = set()\n",
    "    targets = np.matrix(df[[target]])\n",
    "    #print('Min_vars',min_vars)\n",
    "    \n",
    "    saved_results = list()\n",
    "    last_result = None\n",
    "    \n",
    "    def get_result(current_vars, new_var):\n",
    "        temp_vars = current_vars | { new_var }\n",
    "            \n",
    "        data_matrix = pad_ones_column(np.matrix(df[list(temp_vars)]))\n",
    "        (np.matrix(df[list(temp_vars)]))\n",
    "        #print('new_var:', new_var)\n",
    "        #print('temp_vars:',temp_vars)\n",
    "        try:\n",
    "            result = to_minimize(data_matrix, targets, **kwargs)\n",
    "        except:\n",
    "            return np.inf\n",
    "        \n",
    "        #print('Result:',result)\n",
    "        return result\n",
    "    \n",
    "    while len(available_vars) > min_vars:\n",
    "        temp_vars = list(available_vars)\n",
    "        #print('Current vars:', current_vars)\n",
    "        #print('Temp_vars:', temp_vars)\n",
    "            \n",
    "        results = np.fromiter(map(lambda var: get_result(current_vars, var), temp_vars),\n",
    "                             float)\n",
    "        \n",
    "        if last_result is None:\n",
    "            best_result = np.min(results)\n",
    "            best_var = temp_vars[np.argmin(results)]\n",
    "            \n",
    "            last_result = best_result\n",
    "            saved_results.append((best_var, best_result))\n",
    "            \n",
    "            current_vars = { best_var } | current_vars\n",
    "            available_vars = available_vars - { best_var }\n",
    "        else:\n",
    "            best_results = results[(last_result - results)  > eps]\n",
    "            \n",
    "            if len(best_results) > 0:\n",
    "                best_result = np.min(results)\n",
    "                best_var = temp_vars[np.argmin(results)]\n",
    "            \n",
    "                last_result = best_result\n",
    "                saved_results.append((best_var, best_result))\n",
    "                \n",
    "                current_vars = { best_var } | current_vars\n",
    "                available_vars = available_vars - { best_var }\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return saved_results\n",
    "\n",
    "def bayesian_information_criterion(X, y, s_squared_est=True):\n",
    "    # Number of variables used, subtracting the padded ones column\n",
    "    d = X.shape[1] - 1\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    beta_hat = estimate_beta(X, y)\n",
    "    predicted_y = np.matmul(X, beta_hat)\n",
    "    MSE, SSE = run_complete_linear_regression(X, y, beta_hat)\n",
    "\n",
    "    if s_squared_est:\n",
    "        sigma_squared_hat = np.sum(np.square(predicted_y - np.mean(predicted_y))) / (N - 1)\n",
    "    else:\n",
    "        sigma_squared_hat = MSE\n",
    "\n",
    "    t1 = np.power(np.log(2 * np.pi * sigma_squared_hat), N / 2)\n",
    "    t2 = (SSE / (2 * sigma_squared_hat))\n",
    "    t3 = ((d + 2) / N) * np.log(N)\n",
    "    \n",
    "#     bic = t1 + t2 + t3\n",
    "    bic = t2 + t3\n",
    "    \n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(X, y, n_splits=20):\n",
    "    \"\"\"Run k-fold cross validation\"\"\"\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    storeResultsMSE = []\n",
    "    storeResultsSSE = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        beta_hat = estimate_beta(X_train, y_train)\n",
    "\n",
    "        test_MSE, test_SSE = run_complete_linear_regression(X_test, y_test, beta_hat)\n",
    "\n",
    "        storeResultsMSE.append(test_MSE)\n",
    "        storeResultsSSE.append(test_SSE)\n",
    "\n",
    "        #print(\"Iteration:\", i, \"MSE: \", test_MSE, \"SSE:\", test_SSE)  \n",
    "    print(\"Average MSE:\", np.mean(storeResultsMSE), \"Average SSE:\", np.mean(storeResultsSSE) )\n",
    "    return np.mean(storeResultsMSE)\n",
    "    \n",
    "def run_predict_complete_prediction(df, target, variables):\n",
    "    \"\"\"Run the complete prediction workflow,\n",
    "    targeting the given variable using only the variables specified.\"\"\"\n",
    "    \n",
    "    assert len(variables) > 0\n",
    "    \n",
    "    targets = np.matrix(df[[target]])\n",
    "    data_matrix = np.matrix(df[variables])\n",
    "    \n",
    "#     data_matrix = center_scale(data_matrix)\n",
    "    data_matrix = pad_ones_column(data_matrix)\n",
    "    \n",
    "#     targets = center_scale(targets)\n",
    "    \n",
    "    avg_mse = run_k_fold(data_matrix, targets)\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DF = pd.read_csv('train.csv')\n",
    "INDEPENDENT_VARIABLES = [col for col in ORIGINAL_DF.columns if col not in TARGET_VARIABLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:109: RuntimeWarning: overflow encountered in power\n"
     ]
    }
   ],
   "source": [
    "for t in TARGET_VARIABLES:\n",
    "    res = forward_stepwise_selection(ORIGINAL_DF, t, INDEPENDENT_VARIABLES,\n",
    "                           bayesian_information_criterion, 179,50 )\n",
    "    bic = [r[1] for r in res]\n",
    "    x_vals = [y for y in range(0,len(res))]\n",
    "    x_labs = [r[0] for r in res]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x_vals,bic)\n",
    "    plt.ylabel('BIC')\n",
    "    plt.xticks(x_vals,x_labs,rotation = 90)\n",
    "    plt.title('BIC vs Features: '+ t)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('BIC_REG_' + t + '.jpg')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DF = pd.read_csv('train_not_reg.csv')\n",
    "INDEPENDENT_VARIABLES = [col for col in ORIGINAL_DF.columns if col not in TARGET_VARIABLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:111: RuntimeWarning: overflow encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appearance\n",
      "aroma\n",
      "overall\n",
      "palate\n",
      "taste\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_vars = []\n",
    "for t in TARGET_VARIABLES:\n",
    "    res = forward_stepwise_selection(ORIGINAL_DF, t, INDEPENDENT_VARIABLES,\n",
    "                           bayesian_information_criterion, 179,50 )\n",
    "    bic = [r[1] for r in res]\n",
    "    x_vals = [y for y in range(0,len(res))]\n",
    "    x_labs = [r[0] for r in res]\n",
    "    selected_vars.append(x_labs)\n",
    "    fig = plt.figure()\n",
    "    print(t)\n",
    "    plt.plot(x_vals,bic)\n",
    "    plt.ylabel('BIC')\n",
    "    plt.xticks(x_vals,x_labs,rotation = 90)\n",
    "    plt.title('BIC vs Features: '+ t)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('BIC_NOT_REG_' + t + '.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.224659704117 Average SSE: 421.236945219\n",
      "Average MSE: 0.249082230926 Average SSE: 467.029182986\n",
      "Average MSE: 0.311401974429 Average SSE: 583.878702055\n",
      "Average MSE: 0.251150661925 Average SSE: 470.907491109\n",
      "Average MSE: 0.261093929353 Average SSE: 489.551117537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26109392935308068"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model (Reg)\n",
    "# TARGET_VARIABLES = ['appearance', 'aroma', 'overall', 'palate', 'taste']\n",
    "# Appearance\n",
    "run_predict_complete_prediction(ORIGINAL_DF,'appearance',selected_vars[0])\n",
    "\n",
    "# Aroma\n",
    "run_predict_complete_prediction(ORIGINAL_DF,'aroma',selected_vars[1])\n",
    "\n",
    "# Overall\n",
    "run_predict_complete_prediction(ORIGINAL_DF,'overall',selected_vars[2])\n",
    "\n",
    "# Palate\n",
    "run_predict_complete_prediction(ORIGINAL_DF,'palate',selected_vars[3])\n",
    "dfs\n",
    "# Taste\n",
    "run_predict_complete_prediction(ORIGINAL_DF,'taste',selected_vars[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userBias, avg_sent, ABV, #pos_words, Rauchbier, American Double / Imperial Stout, exclamations, #pos/#neg, #neg_words, American Malt Liquor, Netherlands, Euro Strong Lager, Euro Pale Lager, char_count, Poland, Munich Helles Lager, American Pale Ale (APA)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\", \").join(selected_vars[4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
